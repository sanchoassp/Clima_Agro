{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "354bc1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: viewsEnabled\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/24 12:37:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import numpy                 as np\n",
    "import pandas                as pd\n",
    "import glob                  as g\n",
    "from   datetime              import date\n",
    "\n",
    "from pyspark.sql             import SparkSession\n",
    "from pyspark.sql.types       import StringType\n",
    "from pyspark.sql.types       import TimestampType\n",
    "from pyspark.sql             import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions   import *\n",
    "from pyspark.sql             import Window\n",
    "from pyspark.sql.types       import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder.master('local[*]')\n",
    "        .appName('Churn')\n",
    "        .config('spark.driver.memory','2g') \n",
    "        .config('spark.executor.memory','2g')\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .config('spark.ui.showConsoleProgress', True)\n",
    "        .config(\"viewsEnabled\", \"true\")\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa95eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (spark.read.format(\"csv\") \n",
    "      .option(\"header\", \"True\") \n",
    "      .load('../Agro/raw_data/*.csv') \n",
    "      .withColumn(\"filename\", input_file_name())\n",
    "    )\n",
    "\n",
    "df = df.select(\n",
    "                'Date', 'RegionID', 'dewpoint_temperature_2m',\n",
    "                'max_temperature_2m', 'min_temperature_2m','temperature_2m',\n",
    "                'humidity',  'surface_pressure', 'total_precipitation',\n",
    "                'u_component_of_wind_10m', 'v_component_of_wind_10m', \n",
    "                )\n",
    "\n",
    "df = df.withColumnRenamed(\"RegionID\", \"CD_MUN\")\n",
    "\n",
    "df = df.withColumn('max_temperature_2m',df['max_temperature_2m'].cast(FloatType()))\n",
    "df = df.withColumn('min_temperature_2m',df['min_temperature_2m'].cast(FloatType()))\n",
    "df = df.withColumn('humidity',df['humidity'].cast(FloatType()))\n",
    "df = df.withColumn('surface_pressure',df['surface_pressure'].cast(FloatType()))\n",
    "df = df.withColumn('total_precipitation',df['total_precipitation'].cast(FloatType()))\n",
    "df = df.withColumn('u_component_of_wind_10m',df['u_component_of_wind_10m'].cast(FloatType()))\n",
    "df = df.withColumn('v_component_of_wind_10m',df['v_component_of_wind_10m'].cast(FloatType()))\n",
    "\n",
    "\n",
    "d_para = (spark.read.format(\"csv\") \n",
    "      .option(\"header\", \"True\") \n",
    "      .load('../Agro/raw_data/de_para_IBGE.csv') \n",
    "    )\n",
    "\n",
    "d_para = d_para.withColumn('CD_MUN',d_para['CD_MUN'].cast(StringType()))\n",
    "\n",
    "\n",
    "df = (df.join(d_para, (df[\"CD_MUN\"] == d_para[\"CD_MUN\"]), how=\"inner\"))\n",
    "\n",
    "df = (df.groupby('SIGLA_UF','Date')\n",
    "               .agg(F.mean(\"max_temperature_2m\").alias(\"max_temperature_2m\"),\n",
    "                    F.mean(\"min_temperature_2m\").alias(\"min_temperature_2m\"),\n",
    "                    F.mean(\"humidity\").alias(\"humidity\"),\n",
    "                    F.mean(\"surface_pressure\").alias(\"surface_pressure\"),\n",
    "                    F.mean(\"total_precipitation\").alias(\"total_precipitation\"),\n",
    "                    F.mean(\"u_component_of_wind_10m\").alias(\"u_component_of_wind_10m\"),\n",
    "                    F.mean(\"v_component_of_wind_10m\").alias(\"v_component_of_wind_10m\")\n",
    "                   )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b8b9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/24 12:39:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: NM_MUN, SIGLA_UF, , , , , , , \n",
      " Schema: Date, RegionID, humidity, max_temperature_2m, min_temperature_2m, surface_pressure, total_precipitation, u_component_of_wind_10m, v_component_of_wind_10m\n",
      "Expected: Date but found: NM_MUN\n",
      "CSV file: file:///Users/aurelianosancho/Documents/Agro/raw_data/de_para_IBGE.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df.write.format(\"parquet\")\n",
    "    #.mode(\"overwrite\")\n",
    "    .save(\"../Agro/raw_data/clima_Agro.parquet\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
